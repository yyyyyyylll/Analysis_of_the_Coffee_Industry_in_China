from DrissionPage import ChromiumPage, ChromiumOptions
import pandas as pd
import time
import os
import re

# ========== å‚æ•°è®¾ç½® ==========
search_keyword = 'ç‘å¹¸ æ¶ˆè´¹'
image_save_folder = f'å°çº¢ä¹¦_{search_keyword}_å›¾ç‰‡'
csv_input_file = 'å°çº¢ä¹¦_æ±‡æ€»è¯¦æƒ…é“¾æ¥.csv'
csv_output_detail = f'å°çº¢ä¹¦_{search_keyword}_å¸–å­è¯¦æƒ….csv'
csv_output_comments = f'å°çº¢ä¹¦_{search_keyword}_å¸–å­è¯„è®º.csv'
max_comments_to_scrape = 10
batch_write_size = 10


# ========== è¯»å–å‡½æ•° ==========
def robust_read_csv(file_path):
    try:
        df = pd.read_csv(file_path, encoding='utf-8-sig')
        print(" ä½¿ç”¨ utf-8-sig æˆåŠŸè¯»å–æ–‡ä»¶")
        return df
    except Exception:
        try:
            df = pd.read_csv(file_path, encoding='gb18030')
            print(" ä½¿ç”¨ gb18030 æˆåŠŸè¯»å–æ–‡ä»¶")
            return df
        except Exception as e:
            print(f"âŒ æ— æ³•è¯»å– CSVï¼š{e}")
            raise e


# ========== æ–‡ä»¶åæ¸…æ´—==========
def sanitize_filename(name):
    """æ¸…æ´—æ–‡ä»¶åå­—ç¬¦ä¸²"""
    if not name:
        return 'æœªå‘½å'
    sanitized_name = re.sub(r'[\\/*?:"<>|]', "", name)
    sanitized_name = sanitized_name.replace(' ', '_').replace('\n', '_')
    return sanitized_name[:100]


# ========== å†™å…¥CSV==========
def write_data_to_csv(data_list, filename):
    if not data_list:
        return
    header = not os.path.exists(filename)
    df = pd.DataFrame(data_list)
    df.to_csv(filename, mode='a', header=header, index=False, encoding='utf-8-sig')


# ========== åˆå§‹åŒ–æµè§ˆå™¨ ==========
co = ChromiumOptions()
co.set_browser_path(r'C:\Program Files\Google\Chrome\Application\chrome.exe')
dp = ChromiumPage(co)
dp.get('https://www.xiaohongshu.com')
input(" è¯·åœ¨å¼¹å‡ºçš„æµè§ˆå™¨ä¸­æ‰‹åŠ¨å®Œæˆç™»å½•ï¼Œç™»å½•æˆåŠŸåæŒ‰å›è½¦ç»§ç»­...")

# ========== åˆ›å»ºå›¾ç‰‡ä¿å­˜æ–‡ä»¶å¤¹ ==========
os.makedirs(image_save_folder, exist_ok=True)
print(f" å›¾ç‰‡å°†ä¿å­˜åœ¨ '{image_save_folder}' æ–‡ä»¶å¤¹ä¸­ã€‚")

# ========== è¯»å–é“¾æ¥ ==========
try:
    df_links = robust_read_csv(csv_input_file)
    if 'å¸–å­é“¾æ¥' not in df_links.columns:
        raise ValueError("âŒ CSV æ–‡ä»¶ä¸­æœªæ‰¾åˆ° 'å¸–å­é“¾æ¥' åˆ—")
    detail_links = df_links['å¸–å­é“¾æ¥'].dropna().unique().tolist()
    print(f"æˆåŠŸè¯»å– {len(detail_links)} æ¡è¯¦æƒ…é“¾æ¥")
except Exception as e:
    print(f"âŒ æ— æ³•è¯»å–è¾“å…¥æ–‡ä»¶ï¼š{e}")
    exit(1)

# ========== æŠ“å–ä¿¡æ¯ ==========
detail_results = []
comment_results = []
untitled_post_counter = 0

for i, url in enumerate(detail_links):
    try:
        dp.get(url)
        dp.wait.ele_displayed('css:#detail-title', timeout=10)

        # --- å¸–å­è¯¦æƒ…æŠ“å–
        profile_a = dp.ele('xpath://*[@id="noteContainer"]/div[4]/div[1]/div/div[1]/a[1]')
        user_home = profile_a.attr('href') if profile_a else 'æœªæ‰¾åˆ°'
        user_ele = dp.ele('css:span.username')
        user = user_ele.text.strip() if user_ele else ''
        title_ele = dp.ele('css:#detail-title')
        title = title_ele.text.strip() if title_ele else 'æ— æ ‡é¢˜'

        content_element = dp.ele('css:#desc, .desc')
        if content_element:
            full_text = content_element.text
            hashtags = re.findall(r'#\w+', full_text)
            if hashtags:
                tags_str = ', '.join([tag.lstrip('#') for tag in hashtags])
                content = full_text
                for tag in hashtags:
                    content = content.replace(tag, '')
                content = content.strip()
            else:
                content = full_text.strip()
                tags_str = ''
        else:
            content = ''
            tags_str = ''
        if not tags_str:
            tag_eles = dp.eles('css:a#hash-tag')
            tags_str = ', '.join([t.text.strip().lstrip('#') for t in tag_eles])

        post_time = ''
        post_location = ''
        time_ele = dp.ele('css:.date')
        if time_ele:
            full_date_text = time_ele.text.strip()
            parts = full_date_text.split(' ')
            if len(parts) > 1 and re.fullmatch(r'[\u4e00-\u9fa5]+', parts[-1]):
                post_location = parts[-1]
                post_time = ' '.join(parts[:-1])
            else:
                post_time = full_date_text
                post_location = ''

        # --- å›¾ç‰‡ä¸‹è½½
        img_elements = dp.eles('css:div.swiper-slide:not(.swiper-slide-duplicate) img.note-slider-img')
        if img_elements:
            if title and title != 'æ— æ ‡é¢˜':
                base_filename = sanitize_filename(title)
            else:
                untitled_post_counter += 1
                base_filename = f"æ— æ ‡é¢˜{untitled_post_counter}"
            print(f"å‘ç° {len(img_elements)} å¼ å›¾ç‰‡ï¼Œå‡†å¤‡ä¸‹è½½...")
            for j, img in enumerate(img_elements):
                src = img.attr('src')
                if not src: continue
                try:
                    img_response = dp.session.get(src, timeout=20)
                    content_type = img_response.headers.get('Content-Type', 'image/jpeg')
                    ext_map = {'image/jpeg': '.jpg', 'image/png': '.png', 'image/webp': '.webp'}
                    extension = ext_map.get(content_type, '.jpg')
                    image_filename = f"{base_filename}_{j + 1}{extension}"
                    save_path = os.path.join(image_save_folder, image_filename)
                    with open(save_path, 'wb') as f:
                        f.write(img_response.content)
                    print(f"    -    å·²ä¿å­˜å›¾ç‰‡ï¼š{image_filename}")
                except Exception as img_e:
                    print(f"    - âŒ ä¸‹è½½å›¾ç‰‡å¤±è´¥ï¼š{src}ï¼ŒåŸå› ï¼š{img_e}")

        like_span = dp.ele('css:.engage-bar-container .like-wrapper .count')
        like_count = like_span.text.strip() if like_span else '0'
        collect_span = dp.ele('css:.engage-bar-container .collect-wrapper .count')
        collect_count = collect_span.text.strip() if collect_span else '0'
        comment_span = dp.ele('css:.engage-bar-container .chat-wrapper .count')
        comment_count = comment_span.text.strip() if comment_span else '0'
        detail_results.append({
            'è¯¦æƒ…é“¾æ¥': url, 'ç”¨æˆ·': user, 'ç”¨æˆ·ä¸»é¡µ': user_home, 'æ ‡é¢˜': title,
            'æ­£æ–‡': content, 'æ ‡ç­¾': tags_str,
            'å‘å¸ƒæ—¶é—´': post_time, 'å‘å¸ƒåœ°ç‚¹': post_location,
            'ç‚¹èµæ•°': like_count, 'æ”¶è—æ•°': collect_count, 'è¯„è®ºæ•°': comment_count
        })
        print(f"âœ” æˆåŠŸæŠ“å–è¯¦æƒ…ï¼š{title[:30]}...")

        # --- è¯„è®ºæŠ“å–
        if comment_count in ['0', 'æ— ', 'æŠ¢é¦–è¯„']:
            print("ğŸ’¬ è¯¥å¸–å­æ²¡æœ‰è¯„è®ºï¼Œè·³è¿‡ã€‚")
        else:
            scroll_area = dp.ele('css:.note-scroller')
            if not scroll_area:
                print("âš ï¸ æœªæ‰¾åˆ°è¯„è®ºæ»šåŠ¨åŒºåŸŸ '.note-scroller'ï¼Œè·³è¿‡è¯„è®ºæŠ“å–")
            else:
                scroll_area_selector = scroll_area.css_path
                seen_comments = set()
                total_scrolls = 0
                while len(seen_comments) < max_comments_to_scrape and total_scrolls < 20:
                    dp.run_js(
                        f'''let el = document.querySelector('{scroll_area_selector}'); if (el) {{ el.scrollTop = el.scrollHeight; }}''')
                    time.sleep(2.5)
                    total_scrolls += 1
                    comment_blocks = dp.eles('css:div.comment-item')
                    if not comment_blocks: break
                    initial_seen_count = len(seen_comments)
                    for c in comment_blocks:
                        try:
                            commenter_ele = c.ele('css:.author .name')
                            comment_text_ele = c.ele('css:.content .note-text')
                            if not (commenter_ele and comment_text_ele): continue
                            commenter = commenter_ele.text.strip()
                            comment_text = comment_text_ele.text.strip()
                            if not comment_text: continue
                            comment_id = (commenter, comment_text)
                            if comment_id in seen_comments: continue
                            seen_comments.add(comment_id)
                            comment_time_ele = c.ele('css:.info .date > span:first-child')
                            location_ele = c.ele('css:.info .location')
                            like_ele = c.ele('css:.interactions .like .count')
                            comment_results.append({
                                'å¸–å­æ ‡é¢˜': title, 'è¯„è®ºç”¨æˆ·': commenter, 'è¯„è®ºå†…å®¹': comment_text,
                                'è¯„è®ºæ—¶é—´': comment_time_ele.text.strip() if comment_time_ele else '',
                                'è¯„è®ºåœ°ç‚¹': location_ele.text.strip() if location_ele else '',
                                'è¯„è®ºç‚¹èµæ•°': like_ele.text.strip() if like_ele else '0', 'è¯¦æƒ…é“¾æ¥': url
                            })
                            if len(seen_comments) >= max_comments_to_scrape: break
                        except Exception as e:
                            pass
                    if len(seen_comments) == initial_seen_count:
                        print("æ»šåŠ¨åæœªå‘ç°æ–°è¯„è®ºï¼Œç»“æŸæœ¬å¸–ã€‚")
                        break
                    if len(seen_comments) >= max_comments_to_scrape:
                        print(f"å·²æŠ“æ»¡{max_comments_to_scrape}æ¡è¯„è®ºï¼Œæå‰åœæ­¢æ»šåŠ¨ã€‚")
                        break
    except Exception as e:
        print(f"âŒ æŠ“å–å¤±è´¥ï¼š{url}ï¼ŒåŸå› ï¼š{e}")

    # --- æ‰¹æ¬¡å†™å…¥é€»è¾‘ (ä¿æŒä¸å˜) ---
    if (i + 1) % batch_write_size == 0 or (i + 1) == len(detail_links):
        print(f"\nå·²å¤„ç† {i + 1} æ¡å¸–å­ï¼Œè¾¾åˆ°æ‰¹æ¬¡å¤§å°ï¼Œæ­£åœ¨å†™å…¥æ•°æ®...")

        write_data_to_csv(detail_results, csv_output_detail)
        print(f"   - {len(detail_results)} æ¡å¸–å­è¯¦æƒ…å·²è¿½åŠ åˆ° {csv_output_detail}")
        detail_results.clear()

        write_data_to_csv(comment_results, csv_output_comments)
        print(f"   - {len(comment_results)} æ¡è¯„è®ºå·²è¿½åŠ åˆ° {csv_output_comments}")
        comment_results.clear()

        print("...æ‰¹æ¬¡å†™å…¥å®Œæˆã€‚\n")

print(f"\n å…¨éƒ¨ä»»åŠ¡å®Œæˆã€‚")